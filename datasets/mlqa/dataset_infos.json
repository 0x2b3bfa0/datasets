{"mlqa-translate-train_ar": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa-translate-train_ar", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"train": {"name": "train", "num_bytes": 101227245, "num_examples": 78058, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 13144332, "num_examples": 9512, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/mlqa-translate-train.tar.gz": {"num_bytes": 63364123, "checksum": "e510277b0bc3c0639f53839aba8591aebfe803b8b84c5c2ef0513768b86b7cf9"}}, "download_size": 63364123, "dataset_size": 114371577, "size_in_bytes": 177735700}, "mlqa-translate-train_de": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa-translate-train_de", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"train": {"name": "train", "num_bytes": 77996825, "num_examples": 80069, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 10322113, "num_examples": 9927, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/mlqa-translate-train.tar.gz": {"num_bytes": 63364123, "checksum": "e510277b0bc3c0639f53839aba8591aebfe803b8b84c5c2ef0513768b86b7cf9"}}, "download_size": 63364123, "dataset_size": 88318938, "size_in_bytes": 151683061}, "mlqa-translate-train_vi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa-translate-train_vi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"train": {"name": "train", "num_bytes": 97387431, "num_examples": 84816, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 12731112, "num_examples": 10356, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/mlqa-translate-train.tar.gz": {"num_bytes": 63364123, "checksum": "e510277b0bc3c0639f53839aba8591aebfe803b8b84c5c2ef0513768b86b7cf9"}}, "download_size": 63364123, "dataset_size": 110118543, "size_in_bytes": 173482666}, "mlqa-translate-train_zh": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa-translate-train_zh", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"train": {"name": "train", "num_bytes": 55143547, "num_examples": 76285, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 7418070, "num_examples": 9568, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/mlqa-translate-train.tar.gz": {"num_bytes": 63364123, "checksum": "e510277b0bc3c0639f53839aba8591aebfe803b8b84c5c2ef0513768b86b7cf9"}}, "download_size": 63364123, "dataset_size": 62561617, "size_in_bytes": 125925740}, "mlqa-translate-train_es": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa-translate-train_es", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"train": {"name": "train", "num_bytes": 80789653, "num_examples": 81810, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 10718376, "num_examples": 10123, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/mlqa-translate-train.tar.gz": {"num_bytes": 63364123, "checksum": "e510277b0bc3c0639f53839aba8591aebfe803b8b84c5c2ef0513768b86b7cf9"}}, "download_size": 63364123, "dataset_size": 91508029, "size_in_bytes": 154872152}, "mlqa-translate-train_hi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa-translate-train_hi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"train": {"name": "train", "num_bytes": 168117671, "num_examples": 82451, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 22422152, "num_examples": 10253, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/mlqa-translate-train.tar.gz": {"num_bytes": 63364123, "checksum": "e510277b0bc3c0639f53839aba8591aebfe803b8b84c5c2ef0513768b86b7cf9"}}, "download_size": 63364123, "dataset_size": 190539823, "size_in_bytes": 253903946}, "mlqa-translate-test_ar": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa-translate-test_ar", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 5484467, "num_examples": 5335, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/mlqa-translate-test.tar.gz": {"num_bytes": 10075488, "checksum": "d6539141c529849d09e9d5ab95127a20e238e6b24081fa3abbc671af340ee4e7"}}, "download_size": 10075488, "dataset_size": 5484467, "size_in_bytes": 15559955}, "mlqa-translate-test_de": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa-translate-test_de", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 3884332, "num_examples": 4517, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/mlqa-translate-test.tar.gz": {"num_bytes": 10075488, "checksum": "d6539141c529849d09e9d5ab95127a20e238e6b24081fa3abbc671af340ee4e7"}}, "download_size": 10075488, "dataset_size": 3884332, "size_in_bytes": 13959820}, "mlqa-translate-test_vi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa-translate-test_vi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 5998327, "num_examples": 5495, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/mlqa-translate-test.tar.gz": {"num_bytes": 10075488, "checksum": "d6539141c529849d09e9d5ab95127a20e238e6b24081fa3abbc671af340ee4e7"}}, "download_size": 10075488, "dataset_size": 5998327, "size_in_bytes": 16073815}, "mlqa-translate-test_zh": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa-translate-test_zh", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 4831704, "num_examples": 5137, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/mlqa-translate-test.tar.gz": {"num_bytes": 10075488, "checksum": "d6539141c529849d09e9d5ab95127a20e238e6b24081fa3abbc671af340ee4e7"}}, "download_size": 10075488, "dataset_size": 4831704, "size_in_bytes": 14907192}, "mlqa-translate-test_es": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa-translate-test_es", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 3916758, "num_examples": 5253, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/mlqa-translate-test.tar.gz": {"num_bytes": 10075488, "checksum": "d6539141c529849d09e9d5ab95127a20e238e6b24081fa3abbc671af340ee4e7"}}, "download_size": 10075488, "dataset_size": 3916758, "size_in_bytes": 13992246}, "mlqa-translate-test_hi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa-translate-test_hi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 4608811, "num_examples": 4918, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/mlqa-translate-test.tar.gz": {"num_bytes": 10075488, "checksum": "d6539141c529849d09e9d5ab95127a20e238e6b24081fa3abbc671af340ee4e7"}}, "download_size": 10075488, "dataset_size": 4608811, "size_in_bytes": 14684299}, "mlqa_ar_ar": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_ar_ar", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 8216837, "num_examples": 5335, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 808830, "num_examples": 517, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 9025667, "size_in_bytes": 84744717}, "mlqa_ar_de": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_ar_de", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 2132247, "num_examples": 1649, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 358554, "num_examples": 207, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 2490801, "size_in_bytes": 78209851}, "mlqa_ar_vi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_ar_vi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 3235363, "num_examples": 2047, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 283834, "num_examples": 163, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 3519197, "size_in_bytes": 79238247}, "mlqa_ar_zh": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_ar_zh", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 3175660, "num_examples": 1912, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 334016, "num_examples": 188, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 3509676, "size_in_bytes": 79228726}, "mlqa_ar_en": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_ar_en", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 8074057, "num_examples": 5335, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 794775, "num_examples": 517, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 8868832, "size_in_bytes": 84587882}, "mlqa_ar_es": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_ar_es", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 2981237, "num_examples": 1978, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 223188, "num_examples": 161, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 3204425, "size_in_bytes": 78923475}, "mlqa_ar_hi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_ar_hi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 2993225, "num_examples": 1831, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 276727, "num_examples": 186, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 3269952, "size_in_bytes": 78989002}, "mlqa_de_ar": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_de_ar", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 1587005, "num_examples": 1649, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 195822, "num_examples": 207, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 1782827, "size_in_bytes": 77501877}, "mlqa_de_de": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_de_de", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 4274496, "num_examples": 4517, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 477366, "num_examples": 512, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 4751862, "size_in_bytes": 80470912}, "mlqa_de_vi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_de_vi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 1654540, "num_examples": 1675, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 211985, "num_examples": 182, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 1866525, "size_in_bytes": 77585575}, "mlqa_de_zh": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_de_zh", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 1645937, "num_examples": 1621, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 180114, "num_examples": 190, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 1826051, "size_in_bytes": 77545101}, "mlqa_de_en": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_de_en", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 4251153, "num_examples": 4517, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 474863, "num_examples": 512, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 4726016, "size_in_bytes": 80445066}, "mlqa_de_es": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_de_es", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 1678176, "num_examples": 1776, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 166193, "num_examples": 196, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 1844369, "size_in_bytes": 77563419}, "mlqa_de_hi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_de_hi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 1343983, "num_examples": 1430, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 150679, "num_examples": 163, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 1494662, "size_in_bytes": 77213712}, "mlqa_vi_ar": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_vi_ar", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 3164094, "num_examples": 2047, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 226724, "num_examples": 163, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 3390818, "size_in_bytes": 79109868}, "mlqa_vi_de": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_vi_de", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 2189315, "num_examples": 1675, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 272794, "num_examples": 182, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 2462109, "size_in_bytes": 78181159}, "mlqa_vi_vi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_vi_vi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 7807045, "num_examples": 5495, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 715291, "num_examples": 511, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 8522336, "size_in_bytes": 84241386}, "mlqa_vi_zh": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_vi_zh", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 2947458, "num_examples": 1943, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 265154, "num_examples": 184, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 3212612, "size_in_bytes": 78931662}, "mlqa_vi_en": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_vi_en", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 7727204, "num_examples": 5495, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 707925, "num_examples": 511, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 8435129, "size_in_bytes": 84154179}, "mlqa_vi_es": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_vi_es", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 2822481, "num_examples": 2018, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 279235, "num_examples": 189, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 3101716, "size_in_bytes": 78820766}, "mlqa_vi_hi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_vi_hi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 2738045, "num_examples": 1947, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 251470, "num_examples": 177, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 2989515, "size_in_bytes": 78708565}, "mlqa_zh_ar": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_zh_ar", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 1697005, "num_examples": 1912, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 171743, "num_examples": 188, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 1868748, "size_in_bytes": 77587798}, "mlqa_zh_de": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_zh_de", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 1356268, "num_examples": 1621, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 170686, "num_examples": 190, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 1526954, "size_in_bytes": 77246004}, "mlqa_zh_vi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_zh_vi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 1770535, "num_examples": 1943, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 169651, "num_examples": 184, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 1940186, "size_in_bytes": 77659236}, "mlqa_zh_zh": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_zh_zh", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 4324740, "num_examples": 5137, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 433960, "num_examples": 504, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 4758700, "size_in_bytes": 80477750}, "mlqa_zh_en": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_zh_en", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 4353361, "num_examples": 5137, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 437016, "num_examples": 504, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 4790377, "size_in_bytes": 80509427}, "mlqa_zh_es": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_zh_es", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 1697983, "num_examples": 1947, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 134693, "num_examples": 161, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 1832676, "size_in_bytes": 77551726}, "mlqa_zh_hi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_zh_hi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 1547159, "num_examples": 1767, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 180928, "num_examples": 189, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 1728087, "size_in_bytes": 77447137}, "mlqa_en_ar": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_en_ar", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 6641971, "num_examples": 5335, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 621075, "num_examples": 517, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 7263046, "size_in_bytes": 82982096}, "mlqa_en_de": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_en_de", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 4966262, "num_examples": 4517, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 584725, "num_examples": 512, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 5550987, "size_in_bytes": 81270037}, "mlqa_en_vi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_en_vi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 6958087, "num_examples": 5495, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 631268, "num_examples": 511, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 7589355, "size_in_bytes": 83308405}, "mlqa_en_zh": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_en_zh", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 6441614, "num_examples": 5137, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 598772, "num_examples": 504, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 7040386, "size_in_bytes": 82759436}, "mlqa_en_en": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_en_en", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 13787522, "num_examples": 11590, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 1307399, "num_examples": 1148, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 15094921, "size_in_bytes": 90813971}, "mlqa_en_es": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_en_es", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 6074990, "num_examples": 5253, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 545657, "num_examples": 500, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 6620647, "size_in_bytes": 82339697}, "mlqa_en_hi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_en_hi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 6293785, "num_examples": 4918, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 614223, "num_examples": 507, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 6908008, "size_in_bytes": 82627058}, "mlqa_es_ar": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_es_ar", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 1696778, "num_examples": 1978, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 145105, "num_examples": 161, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 1841883, "size_in_bytes": 77560933}, "mlqa_es_de": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_es_de", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 1361983, "num_examples": 1776, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 139968, "num_examples": 196, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 1501951, "size_in_bytes": 77221001}, "mlqa_es_vi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_es_vi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 1707141, "num_examples": 2018, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 172801, "num_examples": 189, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 1879942, "size_in_bytes": 77598992}, "mlqa_es_zh": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_es_zh", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 1635294, "num_examples": 1947, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 122829, "num_examples": 161, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 1758123, "size_in_bytes": 77477173}, "mlqa_es_en": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_es_en", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 4249431, "num_examples": 5253, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 408169, "num_examples": 500, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 4657600, "size_in_bytes": 80376650}, "mlqa_es_es": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_es_es", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 4281273, "num_examples": 5253, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 411196, "num_examples": 500, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 4692469, "size_in_bytes": 80411519}, "mlqa_es_hi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_es_hi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 1489611, "num_examples": 1723, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 178003, "num_examples": 187, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 1667614, "size_in_bytes": 77386664}, "mlqa_hi_ar": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_hi_ar", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 4374373, "num_examples": 1831, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 402817, "num_examples": 186, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 4777190, "size_in_bytes": 80496240}, "mlqa_hi_de": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_hi_de", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 2961556, "num_examples": 1430, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 294325, "num_examples": 163, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 3255881, "size_in_bytes": 78974931}, "mlqa_hi_vi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_hi_vi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 4664436, "num_examples": 1947, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 411654, "num_examples": 177, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 5076090, "size_in_bytes": 80795140}, "mlqa_hi_zh": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_hi_zh", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 4281309, "num_examples": 1767, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 416192, "num_examples": 189, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 4697501, "size_in_bytes": 80416551}, "mlqa_hi_en": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_hi_en", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 11245629, "num_examples": 4918, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 1076115, "num_examples": 507, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 12321744, "size_in_bytes": 88040794}, "mlqa_hi_es": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_hi_es", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 3789337, "num_examples": 1723, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 412469, "num_examples": 187, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 4201806, "size_in_bytes": 79920856}, "mlqa_hi_hi": {"description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n", "citation": "@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n", "homepage": "https://github.com/facebookresearch/MLQA", "license": "", "features": {"context": {"dtype": "string", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answers": {"feature": {"answer_start": {"dtype": "int32", "id": null, "_type": "Value"}, "text": {"dtype": "string", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}, "id": {"dtype": "string", "id": null, "_type": "Value"}}, "supervised_keys": null, "builder_name": "mlqa", "config_name": "mlqa_hi_hi", "version": {"version_str": "1.0.0", "description": null, "datasets_version_to_prepare": null, "major": 1, "minor": 0, "patch": 0}, "splits": {"test": {"name": "test", "num_bytes": 11606982, "num_examples": 4918, "dataset_name": "mlqa"}, "validation": {"name": "validation", "num_bytes": 1115055, "num_examples": 507, "dataset_name": "mlqa"}}, "download_checksums": {"https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip": {"num_bytes": 75719050, "checksum": "246e8089933d13007fe80684d5c5c0713d6834cf8b3b4a0ec7c66f0a0d2baac8"}}, "download_size": 75719050, "dataset_size": 12722037, "size_in_bytes": 88441087}}